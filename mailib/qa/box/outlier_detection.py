"""
AUTHORS
    Thomas Martin
    Oda _____
    Felipe ____

DESCRIPTION
Update 8 jul 2017
    Client Product guidance based on clustering and pca
initial -
    Outlier detection algorithm using Empirical Orthogonal Function (EOF/ PCA) applied to detect non-linear behaviour
    of the monthly volume of a CNPJ client.

    # PERFORM PCA
    # Perform the PCA on the monthly volume data of a specific ramo.
    #  The volume of each client is normalized prior to apply PCA, so it is more specifically a correlation PCA.
    #
    # The PCA permits to transform a MxN matrix into
    # a new MxK subset of K variables called "Principal Components", the number of k components is chosen to explain most of the variance of the original dataset.
    # Another matrix KxN called Eigevenvectors represent the weights of each original variable in relation with each Principal Component
    # In other word: For a given Principal component (e.g.,k=1), high positive weight mean that the variable is highly positively correlated

    # In our case the input dataset matrix (MxN)  called A has M=24 lines corresponding to the 24 months
    # and around N=165 000 columns which are the different client CNPJ.
    # A filter have been applied to drop any client without a complete time serie of volume.
    # therfore a significative part of the client population have been removed.

    # BASIC IDEA
    # The following code show the methodology to create an outlier detection algorithm
    #
    # Firstly, we applied the PCA on the MxN matrix A and obtained a MxK matrix composed by the K Principal Component.
    # (The number of PC to be used is an hyper parameter of this method. But there is an automatic way to select it objectively.)
    # Then, a new MxN matrix B is reconstructed (see inverse_transform) using only the MxK subset.
    # As said earlier, B has the particularity to represent most of the variance of the original dataset.
    #
    # B represent most of the covariance of the original dataset. Its signal captured the main spatio-temporal features
    # responsible for the variation in the original dataset.
    # An analysis of the Eigvenvectors and Principal Components has permited to identify that the pattern captured
    # showed realistic financial temporal behaviours.
    # For example, the first PC captured a trend signal, whereas the second PC captured Christmas time.
    #
    # By substracting the matrix A by B we obtain the residual (MxN) matrix R.
    # In a similar approach, for each client the residual value express the difference between its profit and the expected profit
    # which take into account the general behaviour observed on the branch of activity.
    #
    # The mean of the R matrix for each column N is null.
    # R is then normalize to obtain a value which represent a ratio of the standard deviation of the residual.
    # Finaly, two simple rule have been applied to detect outliers in the residuals.
    # large negative residuals inferior to -1.5 standard deviation was flaged as unexpected lost of profit
    # large positive residuals superior to 1.5 standard deviation was flaged as unexpected gain of profit

    # The author beleive that using a PCA instead of a FFT method to capture residuals offer advantages.
    # Firstly, let's consider a singular event (one representation), such as the world cup. The FFT will fail to
    # capture the signal of this event. Therfore, the event will be detected as an outlier in the subsequent analysis.
    # The increase of profit generated by such events will represent a significant variation of
    # the original dataset. Therfore, a decomposition methods such has PCA will be able to capture such event.
    #
    # The behaviour of our algorithm is easily shown grafically.


    # CODE NOMENCLATURE
    # A <- df_vol
    # eigenvectors <- loadings_vol
    # PCs <- scores_vol
    # B <- reconstruct
    # R <- noise
"""

import seaborn as sns
import matplotlib.cm as cm
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from pandas.tools.plotting import scatter_matrix
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.metrics.pairwise import euclidean_distances
import glob

def getdf(filepath, convert_numeric=False):
    """
    Prepare dataframe
    
    :param inpath: 
    :return: df
    """
    df = pd.read_csv(filepath, sep=',', index_col=1)
    del df['Unnamed: 0']
    del df[ 'ID_EC']
    if convert_numeric:
        df = df.convert_objects(convert_numeric=True)
    df = df.replace(0, np.nan)
    # Get the Volume total by cliente
    # df_vl_total = df_base.filter(regex='VOL_CAPT_20')
    rng = pd.date_range('6/1/2015', periods=len(df.columns), freq='m')
    df = df.T
    df.index = rng
    return df

def getramo(ramo):
    """
    
    
    :param ramo: 
    :return:
        nb_ramo: array with the occurence number of each 
        df_ramo: dataframe 
    """
    # Extract Ramo de Atividade

    # s = extractserie(inpath+'base.csv', "RAMO_ATIV")
    # s.to_csv(inpath+ramo)
    # print('done')
    df_ramo = pd.read_csv(basepath + ramo, encoding='latin1', sep=',', error_bad_lines=False, header=None)
    df_ramo.index = df_ramo[0]
    df_vol.dropna(inplace=True, how='any', axis=1)
    idname_select = np.array(df_vol.columns)
    df_ramo = df_ramo.loc[idname_select.tolist(), :]
    nb_ramo = df_ramo.iloc[:, 1].value_counts()
    return nb_ramo, df_ramo

def performpca(df, nb_pc=5):
    """
    Perform the Principal COmponent ANalysis on Normalized Data
    :param file: 
    :return: pca object 
    """
    # Remove uncomplete series
    print(df.shape)
    normalized=(df-df.mean())/df.std()
    # normalized.plot()
    # plt.show()
    pca = PCA(nb_pc)
    pca.fit(normalized)
    return pca, normalized

def getpara(pca, normalized):
    """
    Get the components of the PCA which are the score and the ladings
    
    :param pca: 
    :param normalized: 
    :return: 
    """
    scores = pca.transform(normalized)
    loadings = pca.components_

    scores = pd.DataFrame(scores, index=normalized.index, columns=[str(i) for i in range(pca.n_components_)])
    loadings = pd.DataFrame(loadings, index=[str(i) for i in range(pca.n_components_)], columns=normalized.columns)
    return scores, loadings

def extractserie(filename, colname):
    """
     Extract a Serie from a big dataframe 
     To avoid memmory problems
    :param filename:
    :param colname:
    :return:
    df
    """
    values = []
    indexes =[]
    chunksize = 1000
    for i, chunk in enumerate(pd.read_csv(filename, chunksize=chunksize, sep=';', encoding='latin1')):
        print(i)
        values.append(chunk.loc[:, colname])
        # indexes.append(chunk.index)
    print('check1')
    df_sel = pd.concat(values, axis=0)
    # df_sel.index = pd.DatetimeIndex(indexes)
    return df_sel

def getquantile(df, low=0.1, high=0.9):
    """
    Get upper andlower quantile of dataframe
    :param df: 
    :param low: 
    :param high: 
    :return: 
    """
    q1 = df.quantile(low)
    q3 = df.quantile(high)
    print(q1)
    print(q3)
    return df[df<q1],df[df>q3]

def compte(df):
    """
    
    Make the compte plot
    :param df: 
    :return: 
    """

    df.value_counts()[:100].plot(kind='bar')
    plt.show()

def common_elements(list1, list2):
    """
    Find the common element of two list
    
    :param list1: 
    :param list2: 
    :return: 
    """
    result = []
    for element in list1:
        if element in list2:
            result.append(element)
    return result

def factor_scatter_matrix(df, factor, palette=None):
    '''Create a scatter matrix of the variables in df, with differently colored
    points depending on the value of df[factor].
    inputs:
        df: pandas.DataFrame containing the columns to be plotted, as well 
            as factor.
        factor: string or pandas.Series. The column indicating which group 
            each row belongs to.
        palette: A list of hex codes, at least as long as the number of groups.
            If omitted, a predefined palette will be used, but it only includes
            9 groups.
    '''
    from scipy.stats import gaussian_kde

    # if isinstance(factor, basestring):
    factor_name = factor #save off the name
    factor = df[factor] #extract column
    df = df.drop(factor_name,axis=1) # remove from df, so it
        # doesn't get a row and col in the plot.

    classes = list(set(factor))

    if palette is None:
        palette = ['#e41a1c', '#377eb8', '#4eae4b',
                   '#994fa1', '#ff8101', '#fdfc33',
                   '#a8572c', '#f482be', '#999999']

    color_map = dict(zip(classes,palette))

    if len(classes) > len(palette):
        raise ValueError('''Too many groups for the number of colors provided.
We only have {} colors in the palette, but you have {}
groups.'''.format(len(palette), len(classes)))

    colors = factor.apply(lambda group: color_map[group])
    scatter_matrix(df,figsize=(10,10),marker='o',c=colors,diagonal=True, alpha=0.5)
    # plt.show()
    #
    # for rc in range(len(df.columns)):
    #     for group in classes:
    #         y = df[factor == group].iloc[rc].values
    #         gkde = gaussian_kde(y)
    #         ind = np.linspace(y.min(), y.max(), 1000)
    #         axarr[rc][rc].plot(ind, gkde.evaluate(ind),c=color_map[group])
    plt.show()
    # return axarr, color_map

def plot_silhouette(X,range_n_clusters):
    for n_clusters in range_n_clusters:
        # Create a subplot with 1 row and 2 columns
        fig, (ax1, ax2) = plt.subplots(1, 2)
        fig.set_size_inches(18, 7)

        # The 1st subplot is the silhouette plot
        # The silhouette coefficient can range from -1, 1 but in this example all
        # lie within [-0.1, 1]
        ax1.set_xlim([-0.1, 1])
        # The (n_clusters+1)*10 is for inserting blank space between silhouette
        # plots of individual clusters, to demarcate them clearly.
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        # Initialize the clusterer with n_clusters value and a random generator
        # seed of 10 for reproducibility.
        clusterer = KMeans(n_clusters=n_clusters, random_state=10)
        cluster_labels = clusterer.fit_predict(X)

        # The silhouette_score gives the average value for all the samples.
        # This gives a perspective into the density and separation of the formed
        # clusters
        silhouette_avg = silhouette_score(X, cluster_labels, sample_size=15)
        print("For n_clusters =", n_clusters,
              "The average silhouette_score is :", silhouette_avg)

        # Compute the silhouette scores for each sample
        sample_silhouette_values = silhouette_samples(X, cluster_labels)

        y_lower = 10
        for i in range(n_clusters):
            # Aggregate the silhouette scores for samples belonging to
            # cluster i, and sort them
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.spectral(float(i) / n_clusters)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                              0, ith_cluster_silhouette_values,
                              facecolor=color, edgecolor=color, alpha=0.7)

            # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

            # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples

        ax1.set_title("The silhouette plot for the various clusters.")
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

        ax1.set_yticks([])  # Clear the yaxis labels / ticks
        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

        # 2nd Plot showing the actual clusters formed
        colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                    c=colors)

        # Labeling the clusters
        centers = clusterer.cluster_centers_
        # Draw white circles at cluster centers
        ax2.scatter(centers[:, 0], centers[:, 1],
                    marker='o', c="white", alpha=1, s=200)

        for i, c in enumerate(centers):
            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50)

        ax2.set_title("The visualization of the clustered data.")
        ax2.set_xlabel("Feature space for the 1st feature")
        ax2.set_ylabel("Feature space for the 2nd feature")

        plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                      "with n_clusters = %d" % n_clusters),
                     fontsize=14, fontweight='bold')

        plt.show()

def clustering_pc(df):
    """
    
    Perform the clustering of the Principal component
    :return:
     label array
     cluster center 
    """
    nb_clusters = 5
    kmeans = KMeans(n_clusters=nb_clusters, random_state=0).fit(df)
    labels = pd.DataFrame(kmeans.labels_,columns=['labels'], index=df.index)
    df_values_labels = pd.concat([df,labels],axis=1,join='outer')

    palette = ['#e41a1c', '#377eb8', '#4eae4b',
                   '#994fa1', '#ff8101', '#fdfc33',
                   '#a8572c', '#f482be', '#999999']
    return labels, kmeans.cluster_centers_

def plot_heatmap(client_life):

    sns.heatmap(client_life, annot=True, cmap='Reds')
    plt.title('Nivel de demanda')
    plt.show()

def plot_real_rec_comp(df_vol, reconstruct, client_on_each_cluster):
    real_curv = df_vol.loc[:, client_on_each_cluster]
    real_curv.columns = ['client 1', 'client 2', 'client 3', 'client 4', 'client 5']
    ax = real_curv.plot(subplots=True)

    reconstruct_curv = reconstruct.loc[:, client_on_each_cluster]
    reconstruct_curv.columns = ['rec client 1', 'rec client 2', 'rec client 3', 'rec client 4', 'rec client 5']
    reconstruct_curv.plot(ax=ax,subplots=True, linestyle='--')
    plt.show()

    def perform_outlier_analysis():
        outlier_sup_mask = noise_norm > 1.5 # std
        outlier_inf_mask = noise_norm < -1.5  # std
        # Plot original volume high loadings

        ranked = loadings_vol.iloc[1, :].sort_values(ascending=False) # Select the compagny influenced by the PC2
        nearzero = ranked[(ranked[:] < 0.000005) & (ranked[:] > -0.000005)]
        plt.figure()
        plt.plot(reconstruct.loc[:, ranked.index[:5]], color='r', label = 'Clients with highest PC2 weights')
        plt.plot(reconstruct.loc[:, nearzero.index[-5:]], color='g', label='Clients with near zero lodings PC2 weights')
        plt.plot(reconstruct.loc[:, ranked.index[-5:]], color='b', label = 'Clients with lowest PC2 weights')
        plt.title('Plot original profit of the 5 clients with the highest PC2 weight (red) and lowest (blue)')
        plt.show()

        # plt.legend()
        plt.plot(scores_vol)
        plt.legend()
        plt.title(
            'Evolution of the Principal Component. PC1(trend), PC2(Christmas), PC3(summer), PC4/5(seasonal). Note: 0 = PC1')
        plt.show()

        # plot variance explained by the First components
        plt.bar(range(len(pca_vol.explained_variance_ratio_)), pca_vol.explained_variance_ratio_ * 100)
        plt.title('Explained Variance by the Principal components in percentage')
        plt.show()

        Plot
        detection
        algorithm
        pc = 0

        plt.plot(norm_vol.loc[:, ranked.index[:5]], '-')
        plt.plot(norm_vol[outlier_sup_mask].loc[:, ranked.index[:5]], '^', c='r')
        plt.plot(norm_vol[outlier_inf_mask].loc[:, ranked.index[:5]], "v", c='b')
        plt.title('Clients with high PC1(trend) positive weights with their respective anomaly flags')
        plt.legend()
        plt.show()

        pc = 1
        ranked = loadings_vol.iloc[pc, :].sort_values(ascending=False)  # Select the compagny influenced by the PC2
        plt.plot(norm_vol.loc[:, ranked.index[:5]], '-')
        plt.plot(norm_vol[outlier_sup_mask].loc[:, ranked.index[:5]], '^', c='r')
        plt.plot(norm_vol[outlier_inf_mask].loc[:, ranked.index[:5]], "v", c='b')
        plt.title('Clients with high PC2(Christmas) positive weights with their respective anomaly flags')
        plt.legend()
        plt.show()

        pc = 2
        ranked = loadings_vol.iloc[pc, :].sort_values(ascending=False)  # Select the compagny influenced by the PC2
        plt.plot(norm_vol.loc[:, ranked.index[:5]], '-')
        plt.plot(norm_vol[outlier_sup_mask].loc[:, ranked.index[:5]], '^', c='r')
        plt.plot(norm_vol[outlier_inf_mask].loc[:, ranked.index[:5]], "v", c='b')
        plt.title('Clients with high PC3(summer) positive weights with their respective anomaly flags')
        plt.legend()
        plt.show()

        pc = 3
        ranked = loadings_vol.iloc[pc, :].sort_values(ascending=False)  # Select the compagny influenced by the PC2
        plt.plot(norm_vol.loc[:, ranked.index[:5]], '-')
        plt.plot(norm_vol[outlier_sup_mask].loc[:, ranked.index[:5]], '^', c='r')
        plt.plot(norm_vol[outlier_inf_mask].loc[:, ranked.index[:5]], "v", c='b')
        plt.title('Clients with high PC4(sesonality) positive weights with their respective anomaly flags')
        plt.legend()
        plt.show()

        # Plot Original, reconstructed and residuals
        pc = 1
        ranked = loadings_vol.iloc[pc, :].sort_values(ascending=False)  # Select the compagny influenced by the PC2
        plt.plot(noise.loc[:, ranked.index[1]], 'k', label='Residual')
        plt.plot(reconstruct.loc[:, ranked.index[1]], 'g', label='Reconstruction')
        plt.plot(df_vol.loc[:, ranked.index[1]], 'b', label='Original')
        plt.title('Plot Original, reconstructed and residuals of the client with the highest PC2 weight')
        plt.legend()
        plt.show()




        # Aply clustering

def main(df_vol):

    # User input
    # interpretation
    interpret = {
        'Vendas': [0, 0, 10, 5, 0],
        'Communicaçao': [0, 0, 10, 5, 8],
        'Gestao': [10, 0, 0, 0, 0],
        'Financeiro': [0, 10, 0, 0, 0],
        'Planejamento': [0, 0, 0, 0, 10]}

    client_on_each_cluster = [215252, 215248, 215242, 215249, 215239] # Client to analyse

    # Get data
    # Apply the principal component and get parameters (eigenvectors and principal components)
    df_vol.dropna(inplace=True,axis=1,)

    # Perform PCA, get components and parameters
    pca_vol,norm_vol = performpca(df_vol, nb_pc=6)
    scores_vol, loadings_vol  = getpara(pca_vol,norm_vol)
    # pd.DataFrame(scores_vol, columns=['Pc1', 'Pc2', 'Pc3', 'Pc4', 'Pc5'])
    # Invert transform and reconstruct a new matrix with the size of the original dataset using the subset fromed by the Principal components
    reconstruct = pca_vol.inverse_transform(scores_vol)
    reconstruct = pd.DataFrame(reconstruct,index= df_vol.index, columns=df_vol.columns)
    reconstruct = reconstruct * df_vol.std()+ df_vol.mean() # Apply back the standard deviation and mean

    # Compare real curve and reconstitution
    plot_real_rec_comp(df_vol, reconstruct, client_on_each_cluster)
    # Calculate the residual and normalize
    noise = df_vol - reconstruct
    noise_norm = (noise - noise.mean()) / noise.std() # note the mean of the residual is already null

    # Create maskto detect value superior or inferior to 1.5 or -1.5 standard deviation respectively
    labels, centers = clustering_pc(loadings_vol.T)
    cluster_mean = pd.DataFrame(centers.dot(scores_vol.T)).T
#    cluster_mean.columns = ['cluster1','cluster2','cluster3','cluster4','cluster5']
    cluster_mean.index =scores_vol.index
    cluster_mean.plot(subplots=True)
    plt.show()

    # Calcul distance to the mean
    dist_to_mean = pd.DataFrame(euclidean_distances(centers, loadings_vol.T.loc[client_on_each_cluster, :]))
    dist_to_mean = np.exp(-dist_to_mean*50)
    dist_to_mean.columns = ['client 1', 'client 2','client 3','client 4', 'client 5']
    dist_to_mean.index = ['cluster1','cluster2','cluster3','cluster4','cluster5']

    # Calcul client_life cycle
    int = pd.DataFrame(interpret)
    int.colums = ['cluster1','cluster2','cluster3','cluster4','cluster5']
    int_norm = int / int.sum(axis=0).values
    client_life = dist_to_mean.T.values.dot(int_norm)
    client_life = pd.DataFrame(client_life, index=dist_to_mean.columns, columns=int_norm.columns)

    # Plot
    plot_heatmap(dist_to_mean)
    plot_heatmap(client_life)


    df_mcc = pd.read_csv("../../data/base_mcc.csv", sep=';', encoding='latin1', index_col=1)
    df_mcc = df_mcc[~df_mcc.index.duplicated(keep='first')]

    df_cluster_mcc = pd.concat([df_mcc, labels], join='inner', axis=1)
    n_cluster = 5

    counts_mcc = []
    for label in range(n_cluster):
        df_cluster = df_cluster_mcc[df_cluster_mcc.loc[:, 'labels'] == label]
        count_mcc = df_cluster.loc[:, 'MCC'].value_counts()
        counts_mcc.append(count_mcc)

    counts_mcc = pd.DataFrame(counts_mcc).T
    counts_mcc.columns = range(n_cluster)
    counts_mcc_select= counts_mcc[counts_mcc > 100].dropna()
    df_norm = counts_mcc_select.T.divide(counts_mcc_select.sum(axis=1))
    df_norm = df_norm.T
    df_norm.columns = ['cluster1','cluster2','cluster3','cluster4','cluster5']
    df_norm.plot(kind='bar', stacked=True, fontsize=10)
    plt.show()
    print('Done')


    # # plot frequency in the cluster
    # count_mcc = count_mcc / len(df_cluster)
    # plt.title('Frequence of each MCC in the cluster ' + str(label))
    # count_mcc.plot(kind='bar',fontsize=6, color='b')
    # plt.show()
    #
    # # plot frequency of the MCC in the entiere dataset
    # count_mcc = count_mcc / len(df_cluster_mcc)
    # plt.title('Frequence of each MCC of the cluster in comparison to the entire dataset' + str(label))
    # count_mcc.plot(kind='bar',fontsize=6, color='r')
    # plt.show()

    # clustering_pc(noise_norm)

    # Plot Silhouette
    # n_clusters = 2
    # plot_silhouette(loadings_vol.T,range(2,n_clusters+1))
    print("Done")

def merge_dfs(files, convert_numeric=False):
    """
    Merge the dataframes of a precise
    :param files: list of strin, path of the files
    :return: 
    """

    dfs = []
    for f in files:
        df = getdf(f, convert_numeric=convert_numeric)
        dfs.append(df)
    # dfs = pd.concat(dfs, join='outer', axis=1)
    return dfs

def addition_dfs(dfs):
    """
    Addition dataframe
    
    :param dfs: 
    :return: 
    """
    dfa = dfs[0]
    for df in dfs[1:]:
       dfa.add(df)
    return dfa

if __name__ =='__main__':
    # User input
    inpath ='../divided/' #
    externalpath='/home/tom/project/wemind/ltm/divided/'

    # select files and put in lists
    vol_files= glob.glob(externalpath+'*VOL_*')
    trans_files = glob.glob(externalpath+'*TRANS*')
    mdr_files = glob.glob(externalpath + '*MDR_*')
    mbc_files = glob.glob(externalpath + '*MBC*')
    fat_files = glob.glob(externalpath + '*FAT*')

    # Select total volume
    vol = "VOL_CAPT_20150.csv"  # volumefile name
    # vol = merge_dfs(vol_files, convert_numeric=False)
    df_vol = getdf(inpath+vol) # get the dataframe
    df_vol.dropna(axis=1,inplace=True)

    # get transaction dataframe
    # Note: I do not know how to calcul the total transaction. The following code make the addition
    # of all the different kind of transactions. We should check how the addition was made in the function "addition_dfs"
    trans_dfs = merge_dfs(trans_files, convert_numeric=False)
    trans = addition_dfs(trans_dfs)

    # mdr = merge_dfs(mdr_files, convert_numeric=False)
    # mbc = merge_dfs(mbc_files, convert_numeric=False)
    # fat = merge_dfs(fat_files, convert_numeric=False)

    print('Done')


    main(df_vol)